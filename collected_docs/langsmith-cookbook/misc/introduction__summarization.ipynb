{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23e76e-00b1-4e7d-8533-cd0aa77e6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchainhub langsmith openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e6792e-21bd-44be-b611-4845a913f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3aa15-8c88-4075-a3e7-5878e01895d4",
   "metadata": {},
   "source": [
    "# Summarization with criteria and pairwise evaluators\n",
    "\n",
    "Here, we will walk through the evaluation workflow for summarization.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Here is a dataset of papers to summarize (into tweets):\n",
    "\n",
    "https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf1f074-6ab9-4031-8e26-2ae2ceda641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Arxiv IDs\n",
    "# phi3, llama3 context extension, jamba, longRope, can llms reason & plan, action learning, roformer, attn is all you need, segment anything, # swin transformer\n",
    "ids = [\n",
    "    \"2404.14219\",\n",
    "    \"2404.19553\",\n",
    "    \"2403.19887\",\n",
    "    \"2402.13753\",\n",
    "    \"2403.04121\",\n",
    "    \"2402.15809\",\n",
    "    \"2104.09864\",\n",
    "    \"1706.03762\",\n",
    "    \"2304.02643\",\n",
    "    \"2111.09883\",\n",
    "]\n",
    "\n",
    "# Load papers\n",
    "docs = []\n",
    "for paper_id in ids:\n",
    "    doc = ArxivLoader(query=paper_id, load_max_docs=1).load()\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d90e821-97ea-49eb-b51f-e3e88ece0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Summarization\n",
    "inputs = [d.page_content for d in docs]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"Paper_Tweet_Generator\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Papers to summarize\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"text\": d} for d in inputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22782477-6507-4689-8963-6127159f1110",
   "metadata": {},
   "source": [
    "#### Chain \n",
    "\n",
    "Here is a summarization chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283409fe-dc17-4133-82e7-742f2976f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_tweet_instructions = (\n",
    "    \"You are an assistant that generates Tweets to distill / summarize\"\n",
    "    \" an academic paper. Ensure the summary: (1) has an engaging title, \"\n",
    "    \" (2) provides a bullet point list of main points from the paper, \"\n",
    "    \" (3) utilizes emojis, (4) includes limitations of the approach, and \"\n",
    "    \" (5) highlights in one sentence the key point or innovation in the paper.\"\n",
    ")\n",
    "\n",
    "human = \"Here is a paper to convert into a Tweet: {paper}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_tweet_instructions), (\"human\", human)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753dbb5-8b11-4bc9-8668-fb757fefff1d",
   "metadata": {},
   "source": [
    "Here we adapt the chain to our dataset examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6289484d-4b82-4c2a-94dd-0fa5472093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def predict_tweet_openai_4o(example: dict):\n",
    "    chat = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "    tweet_generator_openai = prompt | chat | StrOutputParser()\n",
    "    response = tweet_generator_openai.invoke({\"paper\": example[\"text\"]})\n",
    "    return {\"answer\": response}\n",
    "\n",
    "def predict_tweet_command_r(example: dict):\n",
    "    chat = ChatCohere(model=\"command-r\", temperature=0)\n",
    "    tweet_generator_cohere = prompt | chat | StrOutputParser()\n",
    "    \"\"\"Use this for summary evaluation\"\"\"\n",
    "    response = tweet_generator_cohere.invoke({\"paper\": example[\"text\"]})\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465837cd-178d-4d10-b9ad-aa743a44ae8c",
   "metadata": {},
   "source": [
    "#### Evaluator \n",
    "\n",
    "Here we use an evaluator prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/rlm/summary-evaluator\n",
    "\n",
    "https://smith.langchain.com/hub/rlm/summary-accurancy-evaluator\n",
    "\n",
    "This prompt can be forked to add any custom criteria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ed5759-56ed-474f-b61e-ff91fcb27256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "summary_criteria_prompt = hub.pull(\"rlm/summary-evaluator\")\n",
    "summary_accuracy_prompt = hub.pull(\"rlm/summary-accurancy-evaluator\")\n",
    "\n",
    "def text_summary_grader(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple criteria evaluator for text summarization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    summary = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = summary_criteria_prompt | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"summary\": summary})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"summary_engagement_score\", \"score\": score}\n",
    "\n",
    "def text_summary_accuracy_grader(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple accuracy evaluator for text summarization\n",
    "    \"\"\"\n",
    "\n",
    "    # Get summary\n",
    "    paper = example.inputs[\"text\"]\n",
    "    inputs = {\"document\": paper}\n",
    "    summary = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = summary_accuracy_prompt | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"summary\": summary, \"input\": inputs})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"summary_accuracy_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad64a7f-a7f7-4a78-b313-b82e186f684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_tweet_openai_4o,\n",
    "    data=dataset_name,\n",
    "    evaluators=[text_summary_grader,text_summary_accuracy_grader],\n",
    "    experiment_prefix=\"summary-gpt4o\",\n",
    "    metadata={\"variant\": \"paper summary tweet, gpt4o\"},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5372aa-9062-4cef-aceb-bff1518d3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'summary-cmdr-2f7d59bd' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/10075ef4-9b96-46ec-9f4c-cf44e8c1e6c3/compare?selectedSessions=7d46a8cf-059e-4ea4-bc7a-4d67e22800ed\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a3f4d369564b5fa344fb5e96a14302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_tweet_command_r,\n",
    "    data=dataset_name,\n",
    "    evaluators=[text_summary_grader,text_summary_accuracy_grader],\n",
    "    experiment_prefix=\"summary-cmdr\",\n",
    "    metadata={\"variant\": \"paper summary tweet, cmdr\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26cdd6b5-8fb4-45fe-aa82-a33eaeeda337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "def evaluate_pairwise(runs: list, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for pairwise answers to score based on  engagement\n",
    "    \"\"\"\n",
    "\n",
    "    # Store scores\n",
    "    scores = {}\n",
    "    for i, run in enumerate(runs):\n",
    "        scores[run.id] = i\n",
    "\n",
    "    # Runs is the pair of runs for each example\n",
    "    answer_a = runs[0].outputs[\"answer\"]\n",
    "    answer_b = runs[1].outputs[\"answer\"]\n",
    "\n",
    "    # LLM with function call, use highest capacity model\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    grade_prompt = hub.pull(\"rlm/pairwise-evaluation-tweet-summary\")\n",
    "    answer_grader = grade_prompt | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": system_tweet_instructions,\n",
    "            \"answer_a\": answer_a,\n",
    "            \"answer_b\": answer_b,\n",
    "        }\n",
    "    )\n",
    "    score = score[\"Preference\"]\n",
    "\n",
    "    # Map from the score to the run assisnment\n",
    "    if score == 1:  # Assistant A is preferred\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == 2:  # Assistant B is preferred\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103fd226-ad2e-484e-b028-e3b45f2017f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/10075ef4-9b96-46ec-9f4c-cf44e8c1e6c3/compare?selectedSessions=a50748c6-9d55-4463-93ef-8e129dd8485c%2C7d46a8cf-059e-4ea4-bc7a-4d67e22800ed&comparativeExperiment=5b9e56f8-24a9-41a6-8a99-8446dafac6c3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77509a5192254427b6a335198dfe72a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x10ee57450>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "\n",
    "evaluate_comparative(\n",
    "    # Replace the following array with the names or IDs of your experiments\n",
    "    [\"summary-gpt4o-d782b2aa\", \"summary-cmdr-2f7d59bd\"],\n",
    "    evaluators=[evaluate_pairwise],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
